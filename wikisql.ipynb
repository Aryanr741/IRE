{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBSVN-e_GR7I",
        "outputId": "7b4eeae8-7131-464e-d0bc-6575a04e85ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ujson\n",
            "  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ujson\n",
            "Successfully installed ujson-5.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ujson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h88bjI-GR7K"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import gzip\n",
        "import bz2\n",
        "import csv\n",
        "import ujson as json\n",
        "import glob\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F5ZQ6-pGR7K"
      },
      "outputs": [],
      "source": [
        "def write_file(out_file,mkdir=True,binary=False):\n",
        "  if mkdir:\n",
        "    dir = os.path.split(out_file)[0]\n",
        "    if dir:\n",
        "      os.makedirs(dir,exist_ok=True)\n",
        "\n",
        "  if binary:\n",
        "    if out_file.endswith('.gz'):\n",
        "      return gzip.open(out_file,'wb')\n",
        "    elif out_file.endswith('.bz2'):\n",
        "      return bz2.open(out_file,'wb')\n",
        "    else:\n",
        "      return open(out_file,'wb')\n",
        "\n",
        "  else:\n",
        "    if out_file.endswith('.gz'):\n",
        "      return gzip.open(out_file,'wt',encoding='utf-8')\n",
        "    elif out_file.endswith('.bz2'):\n",
        "      return bz2.open(out_file,'wt',encoding='utf-8')\n",
        "    else:\n",
        "      return open(out_file,'w',encoding='utf-8')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHw8uexLGR7L"
      },
      "outputs": [],
      "source": [
        "def read_file(in_file,binary=False,errors=None):\n",
        "\n",
        "  if binary:\n",
        "    if in_file.endswith('.gz'):\n",
        "      return gzip.open(in_file,'rb')\n",
        "    elif in_file.endswith('.bz2'):\n",
        "      return bz2.open(in_file,'rb')\n",
        "    else:\n",
        "      return open(in_file,'rb')\n",
        "\n",
        "  else:\n",
        "    if in_file.endswith('.gz'):\n",
        "      return gzip.open(in_file,'rt',encoding='utf-8',errors=errors)\n",
        "    elif in_file.endswith('.bz2'):\n",
        "      return bz2.open(in_file,'rt',encoding='utf-8',errors=errors)\n",
        "    else:\n",
        "      return open(in_file,'r',encoding='utf-8',errors=errors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RqIWiXJGR7L"
      },
      "outputs": [],
      "source": [
        "def shuffle_blocks(it,*,block_size=20000,rand=random):\n",
        "  assert block_size>=4\n",
        "  block = []\n",
        "  for i in it:\n",
        "    block.append(i)\n",
        "    if len(block)>=block_size:\n",
        "      rand.shuffle(block)\n",
        "      for _ in range(block_size//2):\n",
        "        yield block.pop(-1)\n",
        "\n",
        "  rand.shuffle(block)\n",
        "  for b in block:\n",
        "    yield b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9vM0_TtGR7M"
      },
      "outputs": [],
      "source": [
        "def expand_file(input,file_pattern='*',files=None):\n",
        "  if type(input) is str:\n",
        "    if ':' in input:\n",
        "      input = input.split(':')\n",
        "    else:\n",
        "      input = [input]\n",
        "\n",
        "  all_inputs = []\n",
        "  if files is None:\n",
        "    files = []\n",
        "\n",
        "  for i in input:\n",
        "    print(i)\n",
        "    if i in files:\n",
        "      continue\n",
        "    if os.path.isdir(i):\n",
        "      sub_file = glob.glob(i+\"/**/\"+file_pattern,recursive=True)\n",
        "      sub_file = [f for f in sub_file if not os.path.isdir(f)]\n",
        "      sub_file = [f for f in sub_file if f not in input and f not in files]\n",
        "      all_inputs.extend(sub_file)\n",
        "    else:\n",
        "      all_inputs.append(i)\n",
        "\n",
        "  all_inputs.sort()\n",
        "  return all_inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOLdMRycGR7M"
      },
      "outputs": [],
      "source": [
        "def j_lines(input,files=None,limit=0,report_every=100000,*,errors=None,shuffled=None):\n",
        "  lst = [f for f in expand_file(input,\"*.jsonl*\",files) if not f.endswith('.lock')]\n",
        "\n",
        "  return read_lines(lst,limit=limit,report_every=report_every,errors=errors,shuffled=shuffled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd5yMLaTGR7M"
      },
      "outputs": [],
      "source": [
        "def read_lines(input,limit=0,report_every=100000,*,errors=None,shuffled=None):\n",
        "  count =0\n",
        "  input = expand_file(input)\n",
        "  if shuffled:\n",
        "    if type(shuffled) != random.Random:\n",
        "      shuffled = random.Random()\n",
        "\n",
        "    open_blocks = int(math.ceil(len(input)/32.0))\n",
        "    for open_i in range(open_blocks):\n",
        "      open_files = [read_file(i,errors=errors) for i in input[open_i::open_blocks]]\n",
        "      while len(open_files)>0:\n",
        "        fx = shuffled.randint(0,len(open_files)-1)\n",
        "        next_l = open_files[fx].readline()\n",
        "        if next_l:\n",
        "          yield next_l\n",
        "          count +=1\n",
        "\n",
        "        else:\n",
        "          open_files[fx].close()\n",
        "          del open_files[fx]\n",
        "\n",
        "  else:\n",
        "    for i in input:\n",
        "      with read_file(i,errors=errors) as fp:\n",
        "        for l in fp:\n",
        "          yield l\n",
        "          count +=1\n",
        "          if 0<limit<= count:\n",
        "            return\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWq4Ts1jGR7N"
      },
      "source": [
        "## wikisql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NboobRauGR7O",
        "outputId": "a803b14a-578d-44a7-87d1-780229f1589d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting records\n",
            "  Downloading records-0.5.3-py2.py3-none-any.whl (10 kB)\n",
            "Collecting openpyxl<2.5.0 (from records)\n",
            "  Downloading openpyxl-2.4.11.tar.gz (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tablib>=0.11.4 (from records)\n",
            "  Downloading tablib-3.5.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt (from records)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.10/dist-packages (from records) (2.0.23)\n",
            "Collecting jdcal (from openpyxl<2.5.0->records)\n",
            "  Downloading jdcal-1.4.1-py2.py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: et_xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl<2.5.0->records) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy->records) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy->records) (3.0.1)\n",
            "Building wheels for collected packages: openpyxl, docopt\n",
            "  Building wheel for openpyxl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openpyxl: filename=openpyxl-2.4.11-py2.py3-none-any.whl size=222821 sha256=7713fc63bef088e22cfc7ea4b295f7da389be38c7d718e4b6057678756317994\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/0a/c7/3744d0f62b960ef9ff407e2c38975e058629ace1255e8bd95b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=0df5c1fb6b73f9336d4de5bc31f3646ce9afd040e593adc1c5ca6dfeafa29ee2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built openpyxl docopt\n",
            "Installing collected packages: jdcal, docopt, tablib, openpyxl, records\n",
            "  Attempting uninstall: openpyxl\n",
            "    Found existing installation: openpyxl 3.1.2\n",
            "    Uninstalling openpyxl-3.1.2:\n",
            "      Successfully uninstalled openpyxl-3.1.2\n",
            "Successfully installed docopt-0.6.2 jdcal-1.4.1 openpyxl-2.4.11 records-0.5.3 tablib-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m3e2npNGR7P",
        "outputId": "9c2d3116-feb2-49e7-9636-962084f1468b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sqlalchemy==1.3.23 in /usr/local/lib/python3.10/dist-packages (1.3.23)\n"
          ]
        }
      ],
      "source": [
        "!pip install sqlalchemy==1.3.23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMymQqgaGR7P",
        "outputId": "546dacce-1d66-4fab-acec-d3b863f60929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'WikiSQL'...\n",
            "remote: Enumerating objects: 386, done.\u001b[K\n",
            "remote: Counting objects: 100% (192/192), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 386 (delta 185), reused 154 (delta 154), pack-reused 194\u001b[K\n",
            "Receiving objects: 100% (386/386), 50.72 MiB | 42.12 MiB/s, done.\n",
            "Resolving deltas: 100% (212/212), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/salesforce/WikiSQL.git\n",
        "import tarfile\n",
        "\n",
        "file_path = 'WikiSQL/data.tar.bz2'\n",
        "\n",
        "with tarfile.open(file_path, 'r:bz2') as tar:\n",
        "    tar.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od1cK30TGR7O"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "from WikiSQL.dbengine import DBEngine\n",
        "from WikiSQL.query import Query\n",
        "from WikiSQL.common import count_lines\n",
        "import os\n",
        "import re\n",
        "# from util.line_corpus import jsonl_lines, write_open\n",
        "# from util.args_help import fill_from_args\n",
        "# from datasets.wikisql.tables2agg_classify import write_agg_classify\n",
        "# import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1zKyKmSGR7P"
      },
      "outputs": [],
      "source": [
        "def cell_value(cell,ans):\n",
        "  if cell in ans or cell.lower() in ans:\n",
        "    return True\n",
        "  try:\n",
        "    cell_f = float(cell)\n",
        "    for a in ans:\n",
        "      try:\n",
        "        if cell_f == float(a):\n",
        "          return True\n",
        "      except:\n",
        "        pass\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MteOOzGPGR7P"
      },
      "outputs": [],
      "source": [
        "def classify(data_dir,split,*,exclude_header=False,sep_token='*'):\n",
        "  with write_file(os.path.join(data_dir,f'{split}_agg_classify.jsonl')) as fo:\n",
        "    for l in j_lines(os.path.join(data_dir,f'{split}_agg.jsonl')):\n",
        "      data = json.loads(l)\n",
        "      if not exclude_header:\n",
        "        inst = {'id': data['id'], 'text_a': data['question'],'text_b':f'{sep_token}'.join(data['header']),'label':data['index']}\n",
        "      else:\n",
        "        inst = {'id': data['id'], 'text': data['question'],'label':data['index']}\n",
        "      fo.write(json.dumps(inst)+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHwS9cmvGR7P"
      },
      "outputs": [],
      "source": [
        "def convert_queries(queries,tables,output_file,data_file,*,skip_aggregation=True, show_aggregation=False):\n",
        "  tid2rows = dict()\n",
        "  for l in tables:\n",
        "    data = json.loads(l)\n",
        "    tid = data['id']\n",
        "    header = data['header']\n",
        "    rows_or = data['rows']\n",
        "    rows = []\n",
        "\n",
        "    for r in rows_or:\n",
        "      rows.append([str(cv) for cv in r])\n",
        "\n",
        "    tid2rows[tid] = [[str(h) for h in header]] + rows\n",
        "\n",
        "  with open(data_file, 'w') as f:\n",
        "    json.dump(tid2rows, f)\n",
        "\n",
        "  with write_file(output_file) as fo:\n",
        "    for qid,l in enumerate(queries):\n",
        "      data = json.loads(l)\n",
        "      index = data['sql']['agg']\n",
        "\n",
        "      if skip_aggregation and index!=0:\n",
        "        continue\n",
        "\n",
        "      table_id = data['table_id']\n",
        "      rows = tid2rows[table_id]\n",
        "      qtext = data['question']\n",
        "      target_column = data['sql']['sel']\n",
        "      condition_columns = [colndx for colndx,comp,val in data['sql']['conds']]\n",
        "      answers = data['answer']\n",
        "      rowids = data['rowids'] if 'rowids' in data else None\n",
        "\n",
        "      data = dict()\n",
        "      data['id'] = f'{qid}'\n",
        "      data['question'] = qtext\n",
        "      data['header'] = rows[0]\n",
        "      data['rows'] = rows[1:]\n",
        "      data['target_column'] = target_column\n",
        "      data['condition_columns'] = condition_columns\n",
        "      data['table_id'] = table_id\n",
        "      data['index'] = index\n",
        "\n",
        "      if rowids is not None:\n",
        "        data['target_rows'] = rowids\n",
        "\n",
        "      if index ==0:\n",
        "        answers = [str(ans) for ans in answers]\n",
        "        clean_ans = []\n",
        "\n",
        "        for r in rows[1:]:\n",
        "          if cell_value(r[target_column],answers):\n",
        "            clean_ans.append(r[target_column])\n",
        "\n",
        "        data['answers'] = list(set(clean_ans))\n",
        "\n",
        "      else:\n",
        "        data['answers'] = answers\n",
        "\n",
        "      # if show_aggregation and rowids and len(rowids) >1 and index !=0:\n",
        "\n",
        "      fo.write(json.dumps(data)+'\\n')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQJ8M6syGR7Q"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMsQmlJkGR7Q",
        "outputId": "5efd68f8-0000-4da5-d0c6-ce3f4f4747b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "56355it [07:30, 124.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/train_ans.jsonl.gz\n",
            "data/train.tables.jsonl\n",
            "data/train.tables.jsonl\n",
            "data/train_ans.jsonl.gz\n",
            "data/train_ans.jsonl.gz\n",
            "data/train.tables.jsonl\n",
            "data/train.tables.jsonl\n",
            "data/train_ans.jsonl.gz\n",
            "./train_agg.jsonl\n",
            "./train_agg.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]ERROR:sqlalchemy.pool.impl.NullPool:Exception during reset or similar\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n",
            "    fairy._reset(pool)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n",
            "    pool._dialect.do_rollback(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py\", line 558, in do_rollback\n",
            "    dbapi_connection.rollback()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 139873413885952 and this is thread id 139872938464832.\n",
            "ERROR:sqlalchemy.pool.impl.NullPool:Exception closing connection <sqlite3.Connection object at 0x7f3692f8b440>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n",
            "    fairy._reset(pool)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n",
            "    pool._dialect.do_rollback(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py\", line 558, in do_rollback\n",
            "    dbapi_connection.rollback()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 139873413885952 and this is thread id 139872938464832.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 270, in _close_connection\n",
            "    self._dialect.do_close(connection)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py\", line 564, in do_close\n",
            "    dbapi_connection.close()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 139873413885952 and this is thread id 139872938464832.\n",
            "8421it [00:12, 655.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/dev_ans.jsonl.gz\n",
            "data/dev.tables.jsonl\n",
            "data/dev.tables.jsonl\n",
            "data/dev_ans.jsonl.gz\n",
            "data/dev_ans.jsonl.gz\n",
            "data/dev.tables.jsonl\n",
            "data/dev.tables.jsonl\n",
            "data/dev_ans.jsonl.gz\n",
            "./dev_agg.jsonl\n",
            "./dev_agg.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "37it [00:00, 365.14it/s]ERROR:sqlalchemy.pool.impl.NullPool:Exception during reset or similar\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n",
            "    fairy._reset(pool)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n",
            "    pool._dialect.do_rollback(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py\", line 558, in do_rollback\n",
            "    dbapi_connection.rollback()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 139873413885952 and this is thread id 139872938464832.\n",
            "ERROR:sqlalchemy.pool.impl.NullPool:Exception closing connection <sqlite3.Connection object at 0x7f3692f8b640>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 697, in _finalize_fairy\n",
            "    fairy._reset(pool)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 893, in _reset\n",
            "    pool._dialect.do_rollback(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py\", line 558, in do_rollback\n",
            "    dbapi_connection.rollback()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 139873413885952 and this is thread id 139872938464832.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 270, in _close_connection\n",
            "    self._dialect.do_close(connection)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py\", line 564, in do_close\n",
            "    dbapi_connection.close()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 139873413885952 and this is thread id 139872938464832.\n",
            "15878it [00:34, 460.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/test_ans.jsonl.gz\n",
            "data/test.tables.jsonl\n",
            "data/test.tables.jsonl\n",
            "data/test_ans.jsonl.gz\n",
            "data/test_ans.jsonl.gz\n",
            "data/test.tables.jsonl\n",
            "data/test.tables.jsonl\n",
            "data/test_ans.jsonl.gz\n",
            "./test_agg.jsonl\n",
            "./test_agg.jsonl\n"
          ]
        }
      ],
      "source": [
        "for split in ['train','dev','test']:\n",
        "  orig = os.path.join(\"data/\",f'{split}.jsonl')\n",
        "  db_file = os.path.join(\"data/\",f'{split}.db')\n",
        "  ans_file = os.path.join(\"data/\",f'{split}_ans.jsonl.gz')\n",
        "  tbl_file = os.path.join(\"data/\",f'{split}.tables.jsonl')\n",
        "  engine = DBEngine(db_file)\n",
        "\n",
        "  with open(orig) as fs, write_file(ans_file) as fo:\n",
        "\n",
        "    for ls in tqdm(fs):\n",
        "      try:\n",
        "        eg = json.loads(ls)\n",
        "      except json.JSONDecodeError as e:\n",
        "        continue\n",
        "      sql = eg['sql']\n",
        "      qg = Query.from_dict(sql,ordered=False)\n",
        "      gold = engine.execute_query(eg['table_id'],qg,lower=True)\n",
        "      assert isinstance(gold,list)\n",
        "\n",
        "      eg['answer'] = gold\n",
        "      eg['rowids'] = engine.execute_query_rowid(eg['table_id'],qg,lower=True)\n",
        "      fo.write(json.dumps(eg)+'\\n')\n",
        "\n",
        "  convert_queries(j_lines(ans_file),j_lines(tbl_file),os.path.join(\"./\", f\"{split}_agg.jsonl\"),\"wiki_sql_data_agg.jsonl\",skip_aggregation=False)\n",
        "  convert_queries(j_lines(ans_file),j_lines(tbl_file),os.path.join(\"./\", f\"{split}_lookup.jsonl\"),\"wiki_sql_data_lookup.jsonl\",skip_aggregation=True)\n",
        "  classify(\"./\",split)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p86s4q0fImMH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}