{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOqLF_zci0C8",
        "outputId": "dd124c12-9fa3-4d0e-ae4a-868e730634f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CoU6IxTq3fMt"
      },
      "outputs": [],
      "source": [
        "storage_path = \"/content/drive/MyDrive/ire_project/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fubXifmxgT0Q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from transformers import AlbertConfig, AlbertModel\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from transformers import (\n",
        "    AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer,\n",
        "    XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer,\n",
        "    PreTrainedModel\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s3rD6bDogT0S"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GsLOPZqgT0T",
        "outputId": "428ce84c-e122-435e-c012-7c7e670fb5e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id\n",
            "q40\n",
            "\n",
            "question\n",
            "In which hemisphere does the summer solstice occur in December?\n",
            "\n",
            "header\n",
            "['', 'HEMISPHERE', '', 'ORBITAL EVENT', '', 'MONTH OF OCCURENCE']\n",
            "\n",
            "rows\n",
            "[['In the', 'northern hemisphere', ', the', 'summer solstice', 'occurs in', 'June'], ['In the', 'southern hemisphere', ', the', 'summer solstice', 'occurs in', 'December'], ['In the', 'northern hemisphere', ', the', 'winter solstice', 'occurs in', 'December'], ['In the', 'southern hemisphere', ', the', 'winter solstice', 'occurs in', 'June'], ['In the', 'northern hemisphere', ', the', 'spring equinox', 'occurs in', 'March'], ['In the', 'southern hemisphere', ', the', 'spring equinox', 'occurs in', 'September'], ['In the', 'northern hemisphere', ', the', 'fall equinox', 'occurs in', 'September'], ['In the', 'southern hemisphere', ', the', 'fall equinox', 'occurs in', 'March']]\n",
            "\n",
            "target_column\n",
            "1\n",
            "\n",
            "answers\n",
            "['Southern hemisphere']\n",
            "\n",
            "table_id\n",
            "regents-02\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_path = storage_path + \"data/\"\n",
        "file_name = file_path + \"train_lookup.jsonl\"\n",
        "with open(file_name, \"r\") as file:\n",
        "    # Read each line and parse it as JSON\n",
        "    i=0\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "        # print(data)\n",
        "        for key in data:\n",
        "            print(key)\n",
        "            print(data[key])\n",
        "            print()\n",
        "        i+=1\n",
        "        if(i==1): break\n",
        "    file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c0V41UgXgT0U"
      },
      "outputs": [],
      "source": [
        "def getColRepresentation(header, rows):\n",
        "    colRepresentation = []\n",
        "    cols = [[str(h)] for h in header]\n",
        "    for row in rows:\n",
        "        for ci, cell in enumerate(row):\n",
        "            if cell:  # for sparse table use case\n",
        "                cols[ci].append(str(cell))\n",
        "    for col in cols:\n",
        "        col_rep = ' * '.join(col)\n",
        "        colRepresentation.append(col_rep)\n",
        "    return colRepresentation\n",
        "\n",
        "def getRowRepresentation(header, rows):\n",
        "    rowRepresentation = []\n",
        "    for row in rows:\n",
        "        row_rep = ' * '.join([h + ' : ' + str(c) for h, c in zip(header, row) if c])  # for sparse table use case\n",
        "        rowRepresentation.append(row_rep)\n",
        "    return rowRepresentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1_AnH6LrgT0V"
      },
      "outputs": [],
      "source": [
        "def get_data(file_name, tables_cnt):\n",
        "    # tensor1 = torch.Tensor()\n",
        "    # tensor2 = torch.Tensor()\n",
        "    data = {'row_labels': [], 'row_input':[], 'col_labels': [], 'col_input':[], 'row_queries': [], 'col_queries': []}\n",
        "    tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "    max_seq_length = 128\n",
        "    batch_size = 16\n",
        "    with open(file_name, \"r\") as file:\n",
        "        # Read each line and parse it as JSON\n",
        "        i=0\n",
        "        for line in file:\n",
        "            if(i==tables_cnt): break\n",
        "            table = json.loads(line)\n",
        "            header = table['header']\n",
        "            rows = table['rows']\n",
        "            target_label = table['target_column']\n",
        "            query = table['question']\n",
        "            answers = table['answers']\n",
        "            col_queries = [query]*len(header)\n",
        "            row_queries = [query]*len(rows)\n",
        "            colsRepresentation = getColRepresentation(header, rows)\n",
        "            rowsRepresentation = getRowRepresentation(header, rows)\n",
        "\n",
        "            col_labels = [[0.9, 0.1]]*len(header)\n",
        "            row_labels = [[0.9, 0.1]]*len(rows)\n",
        "            col_labels[target_label] = [0.1,0.9]\n",
        "            for j in range(len(rows)):\n",
        "                if(rows[j][target_label] in answers):\n",
        "                    row_labels[j] = [0.1, 0.9]\n",
        "            data['col_input'].extend(colsRepresentation)\n",
        "            data['row_input'].extend(rowsRepresentation)\n",
        "            data['col_labels'].extend(col_labels)\n",
        "            data['row_labels'].extend(row_labels)\n",
        "            data['col_queries'].extend(col_queries)\n",
        "            data['row_queries'].extend(row_queries)\n",
        "            i+=1\n",
        "    file.close()\n",
        "    col_train_encoding = tokenizer(data['col_input'], data['col_queries'],return_tensors='pt', padding=True, max_length=max_seq_length)\n",
        "    row_train_encoding = tokenizer(data['row_input'], data['row_queries'],return_tensors='pt', padding=True, max_length=max_seq_length)\n",
        "\n",
        "    return row_train_encoding, torch.tensor(data['row_labels']).float(), col_train_encoding, torch.tensor(data['col_labels']).float()\n",
        "\n",
        "\n",
        "def get_data_from_table(header, rows, query):\n",
        "    data = {'row_input':[], 'col_input':[], 'row_queries': [], 'col_queries': []}\n",
        "    tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "    max_seq_length = 128\n",
        "    batch_size = 16\n",
        "    col_queries = [query]*len(header)\n",
        "    row_queries = [query]*len(rows)\n",
        "    colsRepresentation = getColRepresentation(header, rows)\n",
        "    rowsRepresentation = getRowRepresentation(header, rows)\n",
        "    data['col_input'].extend(colsRepresentation)\n",
        "    data['row_input'].extend(rowsRepresentation)\n",
        "    data['col_queries'].extend(col_queries)\n",
        "    data['row_queries'].extend(row_queries)\n",
        "    col_encoding = tokenizer(data['col_input'], data['col_queries'],return_tensors='pt', padding=True, truncation=True, max_length=max_seq_length)\n",
        "    row_encoding = tokenizer(data['row_input'], data['row_queries'],return_tensors='pt', padding=True, truncation=True, max_length=max_seq_length)\n",
        "    return row_encoding, col_encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4o9aeQcgT0V",
        "outputId": "151452c9-8c34-4091-8b80-996eaae6dae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    2,    13,    45,    19,    14,  1637, 15429,    13,    45,   743,\n",
            "         15429,  1637,    13,    45,    13,    15,    14,  1637, 14985,   807,\n",
            "            13,    45,   697, 29230,  1637,    13,    45,  3690,    19,  1637,\n",
            "          1617,    16,  3744,  2940,    13,    45,   295,     3,    19,    56,\n",
            "         15429,   630,    14,   697, 29230,  3744,    19,   356,    60,     3,\n",
            "             0,     0,     0],\n",
            "        [    2,    13,    45,    19,    14,  1637, 15429,    13,    45,   775,\n",
            "         15429,  1637,    13,    45,    13,    15,    14,  1637, 14985,   807,\n",
            "            13,    45,   697, 29230,  1637,    13,    45,  3690,    19,  1637,\n",
            "          1617,    16,  3744,  2940,    13,    45,   356,     3,    19,    56,\n",
            "         15429,   630,    14,   697, 29230,  3744,    19,   356,    60,     3,\n",
            "             0,     0,     0],\n",
            "        [    2,    13,    45,    19,    14,  1637, 15429,    13,    45,   743,\n",
            "         15429,  1637,    13,    45,    13,    15,    14,  1637, 14985,   807,\n",
            "            13,    45,  1559, 29230,  1637,    13,    45,  3690,    19,  1637,\n",
            "          1617,    16,  3744,  2940,    13,    45,   356,     3,    19,    56,\n",
            "         15429,   630,    14,   697, 29230,  3744,    19,   356,    60,     3,\n",
            "             0,     0,     0],\n",
            "        [    2,    13,    45,    19,    14,  1637, 15429,    13,    45,   775,\n",
            "         15429,  1637,    13,    45,    13,    15,    14,  1637, 14985,   807,\n",
            "            13,    45,  1559, 29230,  1637,    13,    45,  3690,    19,  1637,\n",
            "          1617,    16,  3744,  2940,    13,    45,   295,     3,    19,    56,\n",
            "         15429,   630,    14,   697, 29230,  3744,    19,   356,    60,     3,\n",
            "             0,     0,     0],\n",
            "        [    2,    13,    45,    19,    14,  1637, 15429,    13,    45,   743,\n",
            "         15429,  1637,    13,    45,    13,    15,    14,  1637, 14985,   807,\n",
            "            13,    45,  1573,    13,  9629,   251,   396,  1637,    13,    45,\n",
            "          3690,    19,  1637,  1617,    16,  3744,  2940,    13,    45,   285,\n",
            "             3,    19,    56, 15429,   630,    14,   697, 29230,  3744,    19,\n",
            "           356,    60,     3],\n",
            "        [    2,    13,    45,    19,    14,  1637, 15429,    13,    45,   775,\n",
            "         15429,  1637,    13,    45,    13,    15,    14,  1637, 14985,   807,\n",
            "            13,    45,  1573,    13,  9629,   251,   396,  1637,    13,    45,\n",
            "          3690,    19,  1637,  1617,    16,  3744,  2940,    13,    45,   299,\n",
            "             3,    19,    56, 15429,   630,    14,   697, 29230,  3744,    19,\n",
            "           356,    60,     3],\n",
            "        [    2,    13,    45,    19,    14,  1637, 15429,    13,    45,   743,\n",
            "         15429,  1637,    13,    45,    13,    15,    14,  1637, 14985,   807,\n",
            "            13,    45,  1080,    13,  9629,   251,   396,  1637,    13,    45,\n",
            "          3690,    19,  1637,  1617,    16,  3744,  2940,    13,    45,   299,\n",
            "             3,    19,    56, 15429,   630,    14,   697, 29230,  3744,    19,\n",
            "           356,    60,     3],\n",
            "        [    2,    13,    45,    19,    14,  1637, 15429,    13,    45,   775,\n",
            "         15429,  1637,    13,    45,    13,    15,    14,  1637, 14985,   807,\n",
            "            13,    45,  1080,    13,  9629,   251,   396,  1637,    13,    45,\n",
            "          3690,    19,  1637,  1617,    16,  3744,  2940,    13,    45,   285,\n",
            "             3,    19,    56, 15429,   630,    14,   697, 29230,  3744,    19,\n",
            "           356,    60,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1]])} tensor([[0.9000, 0.1000],\n",
            "        [0.9000, 0.1000],\n",
            "        [0.9000, 0.1000],\n",
            "        [0.9000, 0.1000],\n",
            "        [0.9000, 0.1000],\n",
            "        [0.9000, 0.1000],\n",
            "        [0.9000, 0.1000],\n",
            "        [0.9000, 0.1000]]) {'input_ids': tensor([[    2,  1637,    19,    14,  1637,    19,    14,  1637,    19,    14,\n",
            "          1637,    19,    14,  1637,    19,    14,  1637,    19,    14,  1637,\n",
            "            19,    14,  1637,    19,    14,     3,    19,    56, 15429,   630,\n",
            "            14,   697, 29230,  3744,    19,   356,    60,     3,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [    2, 15429,  1637,   743, 15429,  1637,   775, 15429,  1637,   743,\n",
            "         15429,  1637,   775, 15429,  1637,   743, 15429,  1637,   775, 15429,\n",
            "          1637,   743, 15429,  1637,   775, 15429,     3,    19,    56, 15429,\n",
            "           630,    14,   697, 29230,  3744,    19,   356,    60,     3,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [    2,  1637,    13,    15,    14,  1637,    13,    15,    14,  1637,\n",
            "            13,    15,    14,  1637,    13,    15,    14,  1637,    13,    15,\n",
            "            14,  1637,    13,    15,    14,  1637,    13,    15,    14,  1637,\n",
            "            13,    15,    14,     3,    19,    56, 15429,   630,    14,   697,\n",
            "         29230,  3744,    19,   356,    60,     3,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [    2, 14985,   807,  1637,   697, 29230,  1637,   697, 29230,  1637,\n",
            "          1559, 29230,  1637,  1559, 29230,  1637,  1573,    13,  9629,   251,\n",
            "           396,  1637,  1573,    13,  9629,   251,   396,  1637,  1080,    13,\n",
            "          9629,   251,   396,  1637,  1080,    13,  9629,   251,   396,     3,\n",
            "            19,    56, 15429,   630,    14,   697, 29230,  3744,    19,   356,\n",
            "            60,     3],\n",
            "        [    2,  1637,  3690,    19,  1637,  3690,    19,  1637,  3690,    19,\n",
            "          1637,  3690,    19,  1637,  3690,    19,  1637,  3690,    19,  1637,\n",
            "          3690,    19,  1637,  3690,    19,     3,    19,    56, 15429,   630,\n",
            "            14,   697, 29230,  3744,    19,   356,    60,     3,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [    2,  1617,    16,  3744,  2940,  1637,   295,  1637,   356,  1637,\n",
            "           356,  1637,   295,  1637,   285,  1637,   299,  1637,   299,  1637,\n",
            "           285,     3,    19,    56, 15429,   630,    14,   697, 29230,  3744,\n",
            "            19,   356,    60,     3,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])} tensor([[0.9000, 0.1000],\n",
            "        [0.1000, 0.9000],\n",
            "        [0.9000, 0.1000],\n",
            "        [0.9000, 0.1000],\n",
            "        [0.9000, 0.1000],\n",
            "        [0.9000, 0.1000]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "row_train_encoding, row_labels, col_train_encodings, col_labels = get_data(file_name, 1)\n",
        "\n",
        "print(row_train_encoding, row_labels, col_train_encodings, col_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gwD6lh91gT0W"
      },
      "outputs": [],
      "source": [
        "def train(model, train_data, optimiser, criterion, batch_size, train_labels):\n",
        "    train_data.to(device)\n",
        "    train_labels.to(device)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    i=0\n",
        "    start = 0\n",
        "    while(start < len(train_labels)):\n",
        "        end = start + batch_size\n",
        "        if(end > len(train_labels)):\n",
        "            end = len(train_labels)\n",
        "        # batch_size = len(data['input_ids'].to(device))\n",
        "        optimiser.zero_grad()\n",
        "        output = model(train_data['input_ids'][start:end, :], train_data['attention_mask'][start:end, :])\n",
        "        # print(output)\n",
        "        # print(torch.tensor(train_labels[start:end]))\n",
        "        # target = torch.tensor(train_logits[i*batch_size:(i+1)*batch_size, :], dtype=torch.float32).to(device)\n",
        "        loss = criterion(output, torch.softmax(torch.tensor(train_labels)[start:end, :], dim=1, dtype = torch.float32).to(device))\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "        train_loss += loss.item()\n",
        "        pred = output.argmax(dim=1, keepdim=True).to(device)\n",
        "        actual = train_labels[start:end].argmax(dim=1, keepdim=True).to(device)\n",
        "        train_correct += pred.eq(actual.view_as(pred)).sum().item()\n",
        "        i +=1\n",
        "        start = end\n",
        "    train_loss /= len(train_labels)\n",
        "    train_acc = train_correct / len(train_labels)\n",
        "    return train_loss, train_acc\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0UCM_ffQM6Ul"
      },
      "outputs": [],
      "source": [
        "def test(model, test_data, criterion, batch_size, test_labels):\n",
        "    test_data.to(device)\n",
        "    test_labels.to(device)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_correct = 0\n",
        "    i=0\n",
        "    start = 0\n",
        "    logits = []\n",
        "    with torch.no_grad():\n",
        "        while(start < len(test_labels)):\n",
        "            end = start + batch_size\n",
        "            if(end > len(test_labels)):\n",
        "                end = len(test_labels)\n",
        "            output = model(test_data['input_ids'][start:end, :], test_data['attention_mask'][start:end, :].to(device))\n",
        "            # print(output)\n",
        "            # print(torch.tensor(test_labels[start:end]))\n",
        "            # target = torch.tensor(test_logits[i*batch_size:(i+1)*batch_size, :], dtype=torch.float32).to(device)\n",
        "            loss = criterion(output, torch.softmax(torch.tensor(test_labels)[start:end, :], dim=1, dtype = torch.float32).to(device))\n",
        "            # loss.backward()\n",
        "            # optimiser.step()\n",
        "            test_loss += loss.item()\n",
        "            pred = output.argmax(dim=1, keepdim=True).to(device)\n",
        "            for out in output:\n",
        "                logits.append(out[1])\n",
        "            actual = test_labels[start:end].argmax(dim=1, keepdim=True).to(device)\n",
        "            test_correct += pred.eq(actual.view_as(pred)).sum().item()\n",
        "            i +=1\n",
        "            start = end\n",
        "    test_loss /= len(test_labels)\n",
        "    test_acc = test_correct / len(test_labels)\n",
        "    return test_loss, test_acc, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fNeIcc0WgT0X"
      },
      "outputs": [],
      "source": [
        "class RCI_interaction(nn.Module):\n",
        "    def __init__(self, l):\n",
        "        super(RCI_interaction, self).__init__()\n",
        "        albert_config = AlbertConfig(hidden_size=512)\n",
        "        self.albert = AlbertModel(albert_config)\n",
        "        self.net1 = nn.Linear(512, 2)\n",
        "        self.leaky_relu = nn.LeakyReLU(l)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        albert_output = self.albert(input_ids, attention_mask=attention_mask)[0]\n",
        "        output = torch.softmax(self.net1(albert_output[:, 0]), dim=1)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0wrE5FvP0Lhn"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_data, train_labels, optimiser=None, criterion=None, batch_size=16, lr=1e-3, epoch=2, checkpoint_dir=None, resume_from_checkpoint=True, checkpoint_file=None):\n",
        "    if(optimiser is None): optimiser = torch.optim.Adam(model.parameters(), lr)\n",
        "    if(criterion is None): criterion = nn.CrossEntropyLoss()\n",
        "    print(\"Training...........\")\n",
        "    checkpoint_path = checkpoint_dir + checkpoint_file\n",
        "\n",
        "    if resume_from_checkpoint:\n",
        "        if checkpoint_path is None or not os.path.exists(checkpoint_path):\n",
        "            start_epoch = 0\n",
        "            print(\"No saved checkpoints to resume\")\n",
        "        else:\n",
        "            print(\"Checkpoint accessing...........\")\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimiser.load_state_dict(checkpoint['optimiser_state_dict'])\n",
        "            # lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            start_epoch = checkpoint['epoch']\n",
        "            print(f\"Resuming training from epoch {start_epoch}\")\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "    for i in range(start_epoch, epoch):\n",
        "        train_loss, train_acc = train(model, train_data, optimiser, criterion, batch_size, train_labels)\n",
        "        print(f\"Epoch {i}   Train loss: {train_loss}    Train accuracy: {train_acc}\")\n",
        "\n",
        "        if checkpoint_dir is not None:\n",
        "            if(not os.path.exists(checkpoint_dir)):\n",
        "                os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimiser_state_dict': optimiser.state_dict(),\n",
        "            }\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "def test_model(model, test_data, criterion, batch_size, test_labels):\n",
        "    print(\"Evaluating on testing data..................\")\n",
        "    test_loss, test_acc = test(model, test_data, criterion, batch_size, test_labels)\n",
        "    print(f\"Testing loss: {test_loss} Test accuracy:  {test_acc}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ckmTKH3gT0X",
        "outputId": "d5b54a49-bfc3-49f3-e136-ec8a51bfd53f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([24, 56]) torch.Size([8, 2])\n",
            "8\n",
            "Training...........\n",
            "Epoch 0   Train loss: 0.04193515129723903    Train accuracy: 0.8676542010684798\n",
            "Epoch 1   Train loss: 0.0415085686851092    Train accuracy: 0.8705682370082565\n",
            "Epoch 2   Train loss: 0.04132189093254445    Train accuracy: 0.8744536182612919\n"
          ]
        }
      ],
      "source": [
        "row_train_data, row_train_labels, col_train_data, col_train_labels = get_data(file_name, 10000)\n",
        "print(row_train_data['input_ids'].size(), row_labels.size())\n",
        "print(len(row_labels))\n",
        "# print(train_data['input_ids'])\n",
        "# print()\n",
        "# print(labels)\n",
        "row_model_interaction = RCI_interaction(0.1).to(device)\n",
        "optimiser = torch.optim.Adam(row_model_interaction.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_model(model=row_model_interaction, train_data=row_train_data, train_labels=row_train_labels, optimiser=optimiser, criterion=criterion, batch_size=16, lr=1e-3, epoch=3, checkpoint_dir=storage_path+\"checkpoints/\", resume_from_checkpoint=False, checkpoint_file=\"row_model_interaction.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g91xUYtPSg_"
      },
      "outputs": [],
      "source": [
        "file_name = file_path + \"test_lookup.jsonl\"\n",
        "row_test_data, row_test_labels, col_test_data, col_test_labels = get_data(file_name, 100)\n",
        "print(row_test_data['input_ids'].size(), row_labels.size())\n",
        "print(len(row_labels))\n",
        "# print(test_data['input_ids'])\n",
        "# print()\n",
        "# print(labels)\n",
        "# row_model = MLP(0.1).to(device)\n",
        "# optimiser = torch.optim.Adam(row_model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "test_model(model = row_model, test_data = row_test_data, criterion = criterion, batch_size = 16, test_labels = row_test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVm8QoURgT0Y",
        "outputId": "37fd9773-e49c-4d91-e529-4114a13f7948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...........\n",
            "Epoch 0   Train loss: 0.046467066804567976    Train accuracy: 0.6666666666666666\n",
            "Epoch 1   Train loss: 0.04621846963564555    Train accuracy: 0.7\n",
            "Epoch 2   Train loss: 0.045756292343139646    Train accuracy: 0.8333333333333334\n"
          ]
        }
      ],
      "source": [
        "col_model_interaction = RCI_interaction(0.1).to(device)\n",
        "optimiser = torch.optim.Adam(col_model_interaction.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_model(model=col_model_interaction, train_data=col_train_data, train_labels=col_train_labels, optimiser=optimiser, criterion=criterion, batch_size=16, lr=1e-3, epoch=3, checkpoint_dir=storage_path+\"checkpoints/\", resume_from_checkpoint=True, checkpoint_file=\"col_model_interaction.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "teZDOoY1gT0Z"
      },
      "outputs": [],
      "source": [
        "def getLogits(header, rows, query, row_model, col_model):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    batch_size = 16\n",
        "    row_data, col_data = get_data_from_table(header, rows, query)\n",
        "    row_labels = torch.zeros([len(row_data),2])\n",
        "    col_labels = torch.zeros([len(col_data), 2])\n",
        "    _, _, rowsLogits = test(row_model, row_data, criterion, batch_size, row_labels)\n",
        "    _, _, colsLogits = test(col_model, col_data, criterion, batch_size, col_labels)\n",
        "    return rowsLogits, colsLogits\n",
        "\n",
        "def getScores(rowsLogits, colsLogits, top_k):\n",
        "    scores = []\n",
        "    for i in range(len(rowsLogits)):\n",
        "        for j in range(len(colsLogits)):\n",
        "            score = float(rowsLogits[i] + colsLogits[j])\n",
        "            scores.append([i,j,score])\n",
        "    scores.sort(key=lambda x: x[2], reverse=True)\n",
        "    return scores[0:top_k]\n",
        "\n",
        "\n",
        "def getQueryAnswers(query, header, rows, row_model, col_model):\n",
        "    batch_size = 16\n",
        "    max_seq_length=128\n",
        "    rowsLogits, colsLogits = getLogits(header, rows, query, row_model, col_model)\n",
        "\n",
        "    top_k = 3\n",
        "    rci_scores = getScores(rowsLogits, colsLogits, top_k)\n",
        "    # row_scores = getScores(rowsLogits, )\n",
        "\n",
        "    return [{'row_ndx': i, 'col_ndx': j, 'confidence_score': score, 'text': rows[i][j]} for i, j, score in rci_scores]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header = ['Participant', 'Race', 'Date']\n",
        "rows = [['Michael', 'Runathon', 'June 10, 2020'],\n",
        "        ['Mustafa', 'Runathon', 'Sept 3, 2020'],\n",
        "        ['Alfio', 'Runathon', 'Jan 1, 2021'],]\n",
        "answers = getQueryAnswers('Who won the race in June?',header, rows, row_model_interaction, col_model_interaction)\n",
        "print(\"Predicted cells along with their confidence scores and values\")\n",
        "for answer in answers:\n",
        "        print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEJRe24btUXf",
        "outputId": "f6ea5b74-7cd3-4945-adc6-e301acf04b91"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted cells along with their confidence scores and values\n",
            "{'row_ndx': 0, 'col_ndx': 0, 'confidence_score': 0.8965473175048828, 'text': 'Michael'}\n",
            "{'row_ndx': 2, 'col_ndx': 0, 'confidence_score': 0.8920146822929382, 'text': 'Alfio'}\n",
            "{'row_ndx': 0, 'col_ndx': 2, 'confidence_score': 0.8879615068435669, 'text': 'Jun 10, 2021'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaCnFgZFvRHc",
        "outputId": "06982b8e-18f0-4fda-bdc1-3c7a4f0d28b2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.23.5)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RcbsOkbsjVx9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "class pipeline:\n",
        "    def __init__(self, row_model, col_model, file_path, data_file_names, representation_file_names, BM25_k=300, top_k=5):\n",
        "        self.row_model = row_model\n",
        "        self.col_model = col_model\n",
        "        # self.file_path = file_path\n",
        "        for file_name in data_file_names:\n",
        "            self.data_file_names.append(file_path + file_name)\n",
        "        for file_name in representation_file_names:\n",
        "            self.representation_file_names.append(file_path + file_name)\n",
        "        self.BM25_k = BM25_k\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def read_file(self, in_file,binary=False,errors=None):\n",
        "        if binary:\n",
        "            if in_file.endswith('.gz'):\n",
        "                return gzip.open(in_file,'rb')\n",
        "            elif in_file.endswith('.bz2'):\n",
        "                return bz2.open(in_file,'rb')\n",
        "            else:\n",
        "                return open(in_file,'rb')\n",
        "\n",
        "        else:\n",
        "            if in_file.endswith('.gz'):\n",
        "                return gzip.open(in_file,'rt',encoding='utf-8',errors=errors)\n",
        "            elif in_file.endswith('.bz2'):\n",
        "                return bz2.open(in_file,'rt',encoding='utf-8',errors=errors)\n",
        "            else:\n",
        "                return open(in_file,'r',encoding='utf-8',errors=errors)\n",
        "\n",
        "    def pre_process(self, path):\n",
        "        di = {}\n",
        "        punc_pattern = r\"[!\\\"#\\$%&\\'\\(\\)\\*\\+,-\\./:;<=>\\?@\\[\\\\\\]\\^_`{\\|}~]\"\n",
        "        with read_file(path) as fp:\n",
        "            for n1,line in enumerate(fp):\n",
        "                data = json.loads(line)\n",
        "                for k,v in data.items():\n",
        "                    qid = k\n",
        "                    header = v[0]\n",
        "                    rows = v[1:]\n",
        "                    # print(qid,header,rows)\n",
        "                    header1 = []\n",
        "                    for h in header:\n",
        "                        res = re.sub(punc_pattern,' ',h)\n",
        "                        res = re.sub(\"\\s+\",' ',res)\n",
        "                        header1.extend(res.lower().split())\n",
        "\n",
        "                    rows1 = []\n",
        "                    for i in rows:\n",
        "                        for j in i:\n",
        "                            res = re.sub(punc_pattern,' ',j)\n",
        "                            res = re.sub(\"\\s+\",' ',res)\n",
        "                            rows1.extend(res.lower().split())\n",
        "\n",
        "                    header1.extend(rows1)\n",
        "                    # print(header1)\n",
        "                    di[k] = header1\n",
        "            return di\n",
        "\n",
        "\n",
        "    def ranking_docs(self, query,di):\n",
        "        tokenized_query = preprocess_query(query)\n",
        "        bm25 = BM25Okapi(di.values())\n",
        "        scores = bm25.get_scores(tokenized_query)\n",
        "        ranked_documents = dict(sorted(zip(di.keys(), scores), key=lambda x: x[1], reverse=True))\n",
        "        return ranked_documents\n",
        "\n",
        "    def BM25(self, query):\n",
        "        top = self.BM25_k\n",
        "        paths = self.data_file_names\n",
        "        di1 = pre_process(paths[0])\n",
        "        di2 = pre_process(paths[1])\n",
        "        di3 = pre_process(paths[2])\n",
        "        ranked_doc1 = ranking_docs(query,di1)\n",
        "        ranked_doc2 = ranking_docs(query,di2)\n",
        "        ranked_doc3 = ranking_docs(query,di3)\n",
        "        result = {**ranked_doc1,**ranked_doc2,**ranked_doc3}\n",
        "        final_result = dict(list(sorted(result.items(), key=lambda x: x[1], reverse=True))[:top])\n",
        "\n",
        "        tables = {}\n",
        "        for i in paths:\n",
        "            with read_file(i) as fp:\n",
        "                for n1,line in enumerate(fp):\n",
        "                    data = json.loads(line)\n",
        "                    for k,v in data.items():\n",
        "                        if(k in final_result.keys()):\n",
        "                            tables[k] = v\n",
        "\n",
        "        return tables\n",
        "\n",
        "    def getColRepresentation(self, header, rows):\n",
        "        colRepresentation = []\n",
        "        cols = [[str(h)] for h in header]\n",
        "        for row in rows:\n",
        "            for ci, cell in enumerate(row):\n",
        "                if cell:  # for sparse table use case\n",
        "                    cols[ci].append(str(cell))\n",
        "        for col in cols:\n",
        "            col_rep = ' * '.join(col)\n",
        "            colRepresentation.append(col_rep)\n",
        "        return colRepresentation\n",
        "\n",
        "    def getRowRepresentation(self, header, rows):\n",
        "        rowRepresentation = []\n",
        "        for row in rows:\n",
        "            row_rep = ' * '.join([h + ' : ' + str(c) for h, c in zip(header, row) if c])  # for sparse table use case\n",
        "            rowRepresentation.append(row_rep)\n",
        "        return rowRepresentation\n",
        "\n",
        "    def get_data_from_table(self, header, rows, query):\n",
        "        data = {'row_input':[], 'col_input':[], 'row_queries': [], 'col_queries': []}\n",
        "        tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "        max_seq_length = 128\n",
        "        batch_size = 16\n",
        "        col_queries = [query]*len(header)\n",
        "        row_queries = [query]*len(rows)\n",
        "        colsRepresentation = getColRepresentation(header, rows)\n",
        "        rowsRepresentation = getRowRepresentation(header, rows)\n",
        "        data['col_input'].extend(colsRepresentation)\n",
        "        data['row_input'].extend(rowsRepresentation)\n",
        "        data['col_queries'].extend(col_queries)\n",
        "        data['row_queries'].extend(row_queries)\n",
        "        col_encoding = tokenizer(data['col_input'], data['col_queries'],return_tensors='pt', padding=True, truncation=True, max_length=max_seq_length)\n",
        "        row_encoding = tokenizer(data['row_input'], data['row_queries'],return_tensors='pt', padding=True, truncation=True, max_length=max_seq_length)\n",
        "        return row_encoding, col_encoding\n",
        "\n",
        "    def getLogits(self, header, rows, query):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        batch_size = 16\n",
        "        row_data, col_data = get_data_from_table(header, rows, query)\n",
        "        row_labels = torch.zeros([len(row_data),2])\n",
        "        col_labels = torch.zeros([len(col_data), 2])\n",
        "        _, _, rowsLogits = test(self.row_model, row_data, criterion, batch_size, row_labels)\n",
        "        _, _, colsLogits = test(self.col_model, col_data, criterion, batch_size, col_labels)\n",
        "        return rowsLogits, colsLogits\n",
        "\n",
        "    def getScores(self, rowsLogits, colsLogits, top_k):\n",
        "        scores = []\n",
        "        for i in range(len(rowsLogits)):\n",
        "            for j in range(len(colsLogits)):\n",
        "                score = float(rowsLogits[i] + colsLogits[j])\n",
        "                scores.append([i,j,score])\n",
        "        scores.sort(key=lambda x: x[2], reverse=True)\n",
        "        return scores[0:top_k]\n",
        "\n",
        "    def RCI(self, query, header, rows):\n",
        "        batch_size = 16\n",
        "        max_seq_length=128\n",
        "        rowsLogits, colsLogits = getLogits(header, rows, query, row_model, col_model)\n",
        "\n",
        "        top_k = 5\n",
        "        rci_scores = getScores(rowsLogits, colsLogits, top_k)\n",
        "        return [{'row_ndx': i, 'col_ndx': j, 'confidence_score': score, 'text': rows[i][j]} for i, j, score in rci_scores]\n",
        "\n",
        "    def RCI_System(self, query):\n",
        "        tables = BM25(query)\n",
        "        table_dict = {}\n",
        "        table_scores = {}\n",
        "        results = {}\n",
        "        for table in tables:\n",
        "            header = table['header']\n",
        "            rows = table['rows']\n",
        "            id = table['id']\n",
        "            table_dict[id] = {'header':header, 'rows': rows}\n",
        "            cell_score = RCI(query, header, rows)\n",
        "            results[id] = cell_score\n",
        "            table_scores[id] = cell_score[0]['confidence_score']\n",
        "\n",
        "        result_table_ids = sorted(table_scores, key=lambda k: table_scores[k], reverse=True)[:top_k]\n",
        "        retrieved_results = []\n",
        "        for id in result_table_ids:\n",
        "            retrieved_results.append({'id':id, 'header':table_dict[id]['header'], 'rows':table_dict[id]['rows'], 'cells': results[id]})\n",
        "        return retrieved_results\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = storage_path + \"data/\"\n",
        "file_name = file_path + \"test_lookup.jsonl\"\n",
        "tables = []\n",
        "with open(file_name, \"r\") as file:\n",
        "    # Read each line and parse it as JSON\n",
        "    i=0\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "        tables.append(data)\n",
        "        # print(data)\n",
        "        # for key in data:\n",
        "        #     print(key)\n",
        "        #     print(data[key])\n",
        "        #     print()\n",
        "        # i+=1\n",
        "        # if(i==1): break\n",
        "    file.close()\n",
        "\n",
        "\n",
        "print(tables[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lqmBBWku-yE",
        "outputId": "a43e7070-f3e5-485f-be11-0abb39790dec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'q20', 'question': 'Which orbital event has midrange day and night periods and happens around Easter?', 'header': ['', 'ORBITAL EVENT', '', 'PERIOD OF DAYLIGHT', '', 'PERIOD OF NIGHT', ''], 'rows': [['The', 'summer solstice', 'is the day with the', 'longest', 'period of daylight and the', 'shortest', 'period of night'], ['The', 'winter solstice', 'is the day with the', 'shortest', 'period of daylight and the', 'longest', 'period of night'], ['The', 'spring equinox', 'is the day with the', 'midrange', 'period of daylight and the', 'midrange', 'period of night'], ['The', 'fall equinox', 'is the day with the', 'midrange', 'period of daylight and the', 'midrange', 'period of night']], 'target_column': 1, 'answers': ['Spring equinox'], 'table_id': 'regents-01'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "rci_answers = []\n",
        "true_answers = []\n",
        "for table in tables[:25]:\n",
        "    true_answers.extend(table['answers'])\n",
        "    start_time = time.time()\n",
        "    answers = getQueryAnswers(table['question'], table['header'], table['rows'], row_model_interaction, col_model_interaction)\n",
        "    end_time = time.time()\n",
        "    rci_answer = []\n",
        "    for answer in answers:\n",
        "        rci_answer.append(answer['text'])\n",
        "    rci_answers.append(rci_answer)\n",
        "\n",
        "total_time = end_time-start_time\n",
        "print(rci_answers)\n",
        "print(true_answers)\n",
        "print(\"time per query is \", total_time/(len(true_answers)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TWmY6FabMJf",
        "outputId": "f89c64eb-8227-4ed1-be47-e93bc8cca6db"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['spring equinox', 'fall equinox', 'summer solstice', 'winter solstice', 'The'], ['period of night', 'period of night', 'period of daylight and the', 'period of daylight and the', 'midrange'], ['midrange', 'shortest', 'longest', 'midrange', 'period of night'], ['midrange', 'longest', 'shortest', 'is the day with the', 'The'], ['midrange', 'period of daylight and the', 'period of night', 'midrange', 'is the day with the'], ['midrange', 'shortest', 'longest', 'midrange', 'midrange'], ['midrange', 'shortest', 'longest', 'midrange', 'longest'], ['midrange', 'period of night', 'midrange', 'midrange', 'period of daylight and the'], ['spring equinox', 'fall equinox', 'The', 'The', 'summer solstice'], ['fall equinox', 'spring equinox', 'The', 'The', 'winter solstice'], ['is the day with the', 'is the day with the', 'is the day with the', 'is the day with the', 'The'], ['midrange', 'midrange', 'period of daylight and the', 'period of night', 'longest'], ['midrange', 'longest', 'shortest', 'The', 'is the day with the'], ['midrange', 'midrange', 'longest', 'shortest', 'period of night'], ['midrange', 'midrange', 'period of daylight and the', 'midrange', 'period of night'], ['midrange', 'period of night', 'midrange', 'midrange', 'period of daylight and the'], ['midrange', 'midrange', 'midrange', 'period of night', 'period of daylight and the'], ['In the', ', the', 'occurs in', 'June', 'summer solstice'], ['occurs in', ', the', 'In the', 'summer solstice', 'June'], ['June', 'occurs in', 'In the', ', the', 'summer solstice'], ['southern hemisphere', 'southern hemisphere', 'equatorial region', 'equatorial region', 'southern hemisphere'], ['South Orkney Islands (Antarctic Treaty signatories)', 'East Timor', 'South Shetland Islands (Antarctic Treaty signatories)', 'American Samoa (United States)', 'Jarvis Island (United States)'], ['southern hemisphere', 'East Timor', 'is located in the', 'southern hemisphere', 'southern hemisphere'], ['southern hemisphere', 'southern hemisphere', 'is located in the', 'is located in the', 'South Africa'], ['southern hemisphere', 'is located in the', 'South Africa', 'southern hemisphere', 'southern hemisphere']]\n",
            "['Spring equinox', 'spring equinox', 'midrange', 'midrange', 'midrange', 'midrange', 'midrange', 'Midrange', 'Fall Equinox and Spring Equinox', 'fall equinox', 'fall equinox', 'Midrange', 'midrange', 'midrange', 'midrange', 'midrange', 'midrange between day and night', 'northern hemisphere', 'Summer solstice', 'June', 'southern', 'East Timor', 'southern hemisphere', 'Southern', 'southern hemisphere']\n",
            "time per query is  0.4549198436737061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hit_at_1 = 0\n",
        "for i in range(len(true_answers)):\n",
        "    if(true_answers[i] == rci_answers[i][0]): hit_at_1 +=1\n",
        "hit_at_1/= len(true_answers)\n",
        "print(hit_at_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPkiKJpXclBh",
        "outputId": "20045810-92fa-483a-8fe0-6ab828c2a477"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reciprocal_rank_sum = 0\n",
        "for i in range(len(true_answers)):\n",
        "    for j in range(len(rci_answers[i])):\n",
        "        if(true_answers[i] == rci_answers[i][j]):\n",
        "            print(1/(j+1))\n",
        "            reciprocal_rank_sum += 1/(j+1)\n",
        "            break\n",
        "mrr = reciprocal_rank_sum/len(true_answers)\n",
        "print(mrr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFwpYreeo3NW",
        "outputId": "175815a0-a20c-4663-b9f4-b551b9d5e15b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.5\n",
            "1.0\n",
            "1.0\n",
            "0.54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gKWs3GtTRD8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TapasForQuestionAnswering\n",
        "import pandas as pd\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
        "model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
        "\n",
        "def get_answers_from_tapas(table):\n",
        "    rows = table['rows']\n",
        "    header = table['header']\n",
        "    query = table['question']\n",
        "\n",
        "    df = pd.DataFrame(rows, columns = header)\n",
        "\n",
        "    inputs = tokenizer(table=df, queries=query, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    logits_aggregation = outputs.logits_aggregation\n",
        "\n",
        "\n",
        "    flat_list = []\n",
        "    for values in rows:\n",
        "        # flat_list.append(key)\n",
        "        for value in values:\n",
        "            flat_list.append(value)\n",
        "    # print(len(flat_list))\n",
        "    total_cells = len(flat_list)\n",
        "    logits = logits[0][:total_cells]\n",
        "    indexes = sorted(range(len(logits)), key=lambda i: logits[i], reverse=True)[:5]\n",
        "    # print(indexes)\n",
        "\n",
        "\n",
        "    # pred = torch.argmax(logits[:, :total_cells], dim=1)\n",
        "    # print(pred)\n",
        "    answers = []\n",
        "    for idx in indexes:\n",
        "        # print(flat_list[idx])\n",
        "        answers.append(flat_list[idx])\n",
        "    return answers\n"
      ],
      "metadata": {
        "id": "e7Ov41gg4fRN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "tapas_answers = []\n",
        "true_answers = []\n",
        "start_time = time.time()\n",
        "for table in tables[:25]:\n",
        "    true_answers.extend(table['answers'])\n",
        "    answers = get_answers_from_tapas(table)\n",
        "    tapas_answers.append(answers)\n",
        "end_time = time.time()\n",
        "\n",
        "total_time = end_time-start_time\n",
        "print(tapas_answers)\n",
        "print(true_answers)\n",
        "print(\"time per query is \", total_time/(len(true_answers)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0S2AdowRV0F",
        "outputId": "14134c35-e359-4d06-bb6e-334088643b28"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['period of night', 'period of daylight and the', 'midrange', 'is the day with the', 'midrange'], ['The', 'summer solstice', 'is the day with the', 'longest', 'period of daylight and the'], ['period of night', 'The', 'summer solstice', 'is the day with the', 'longest'], ['period of daylight and the', 'midrange', 'period of night', 'The', 'summer solstice'], ['midrange', 'period of daylight and the', 'midrange', 'period of night', 'is the day with the'], ['The', 'summer solstice', 'is the day with the', 'longest', 'period of daylight and the'], ['The', 'summer solstice', 'is the day with the', 'longest', 'period of daylight and the'], ['period of night', 'The', 'fall equinox', 'midrange', 'period of daylight and the'], ['period of night', 'The', 'The', 'summer solstice', 'is the day with the'], ['period of daylight and the', 'midrange', 'period of night', 'is the day with the', 'midrange'], ['The', 'summer solstice', 'is the day with the', 'longest', 'period of daylight and the'], ['spring equinox', 'is the day with the', 'midrange', 'fall equinox', 'is the day with the'], ['period of daylight and the', 'midrange', 'period of night', 'The', 'summer solstice'], ['The', 'summer solstice', 'is the day with the', 'longest', 'period of daylight and the'], ['fall equinox', 'is the day with the', 'midrange', 'The', 'summer solstice'], ['period of night', 'The', 'fall equinox', 'midrange', 'period of daylight and the'], ['The', 'fall equinox', 'is the day with the', 'period of daylight and the', 'midrange'], ['northern hemisphere', ', the', 'spring equinox', 'occurs in', 'September'], ['occurs in', 'March', 'September', 'In the', 'In the'], ['occurs in', 'March', 'In the', 'southern hemisphere', ', the'], ['southern hemisphere', 'Andorra', 'Bahrain', 'is located in the', 'Jamaica'], ['Niger', 'is located in the', 'is located in the', 'southern hemisphere', 'Bangladesh'], ['is located in the', 'northern hemisphere', 'southern hemisphere', 'Cayman Islands (UK)', 'Puerto Rico (US)'], ['southern hemisphere', 'Slovenia', 'Libya', 'is located in the', 'is located in the'], ['is located in the', 'northern hemisphere', 'is located in the', 'northern hemisphere', 'is located in the']]\n",
            "['Spring equinox', 'spring equinox', 'midrange', 'midrange', 'midrange', 'midrange', 'midrange', 'Midrange', 'Fall Equinox and Spring Equinox', 'fall equinox', 'fall equinox', 'Midrange', 'midrange', 'midrange', 'midrange', 'midrange', 'midrange between day and night', 'northern hemisphere', 'Summer solstice', 'June', 'southern', 'East Timor', 'southern hemisphere', 'Southern', 'southern hemisphere']\n",
            "time per query is  1.9328322410583496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hit_at_1 = 0\n",
        "for i in range(len(true_answers)):\n",
        "    if(true_answers[i] == tapas_answers[i][0]): hit_at_1 +=1\n",
        "hit_at_1/= len(true_answers)\n",
        "print(hit_at_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG8oEs-5WAD4",
        "outputId": "7c4b6650-9256-4ccf-fb62-72f698655449"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reciprocal_rank_sum = 0\n",
        "for i in range(len(true_answers)):\n",
        "    for j in range(len(tapas_answers[i])):\n",
        "        if(true_answers[i] == tapas_answers[i][j]):\n",
        "            # print(1/(j+1))\n",
        "            reciprocal_rank_sum += 1/(j+1)\n",
        "            break\n",
        "mrr = reciprocal_rank_sum/len(true_answers)\n",
        "print(mrr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpjLInkmWG15",
        "outputId": "547aeb27-b552-4f36-9a1f-fa96dc492233"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TapasForQuestionAnswering\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")"
      ],
      "metadata": {
        "id": "M4O2Ar503gL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "def find_answers(table, mrc_model_name='distilbert-base-cased-distilled-squad'):\n",
        "    query = table['question']\n",
        "    rows_ = table['rows']\n",
        "    header_ = table['header']\n",
        "    rows = \"\"\n",
        "    for i in range(len(rows)):\n",
        "        for j in range(len(rows[0])):\n",
        "            rows += rows[i][j]\n",
        "            if(j<len(rows[0])-1):\n",
        "                rows += \"\\t\"\n",
        "        if(i< len(rows)-1): rows += \"\\n\"\n",
        "\n",
        "    header = \"\"\n",
        "    for j in range(len(header)):\n",
        "        rows += header[j]\n",
        "        if(j<len(header)-1): header += \"\\t\";\n",
        "    # Load the MRC tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(mrc_model_name)\n",
        "\n",
        "    # Tokenize the query, rows, and header separately\n",
        "    question_tokens = tokenizer(query, return_tensors=\"pt\")\n",
        "    context_tokens = tokenizer(f\"{header}\\n{rows}\", return_tensors=\"pt\")\n",
        "\n",
        "    # Combine the tokenized question and context\n",
        "    inputs = {\n",
        "        'input_ids': torch.cat([question_tokens['input_ids'], context_tokens['input_ids']], dim=1),\n",
        "        'attention_mask': torch.cat([question_tokens['attention_mask'], context_tokens['attention_mask']], dim=1)\n",
        "    }\n",
        "\n",
        "    # Load the MRC model\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(mrc_model_name)\n",
        "\n",
        "    # Get model outputs\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Decode the answer from the model output\n",
        "    answer_start = torch.argmax(outputs.start_logits)\n",
        "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "    answer = tokenizer.decode(inputs['input_ids'][0][answer_start:answer_end])\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "# query = \"What is the capital of France?\"\n",
        "# header = \"Country\\tCapital\\n\"\n",
        "# rows = \"France\\tParis\\nGermany\\tBerlin\\nItaly\\tRome\"\n",
        "\n",
        "# answer = find_answers(query, rows, header)\n",
        "# print(answer)\n"
      ],
      "metadata": {
        "id": "hQ4Oxb5g2Tj9"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "bert_answers = []\n",
        "true_answers = []\n",
        "start_time = time.time()\n",
        "for table in tables[:25]:\n",
        "    true_answers.extend(table['answers'])\n",
        "    answers = find_answers(table)\n",
        "    bert_answers.append(answers)\n",
        "end_time = time.time()\n",
        "\n",
        "total_time = end_time-start_time\n",
        "print(bert_answers)\n",
        "print(true_answers)\n",
        "print(\"time per query is \", total_time/(len(true_answers)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTn6wBln6A72",
        "outputId": "49e7be73-5dbe-496e-9450-be0bcc9a0f73"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Easter', 'fall equinox', 'daylight', 'daylight', 'equinox', 'night', 'spring equinox also has _ _ _ _ _ _ _ periods for both daylight and night', 'equinox', 'daylight and period of night', 'midrange period of night', 'daylight', '', 'daylight', 'fall equinox', 'the period of night of the fall equinox', 'equinox', 'equinox', 'summer solstice occurs in June', 'June', '', 'Heard Island', '[CLS] What country is located in the southern hemisphere? [SEP] [CLS]', 'East Timor', 'South Africa', 'South Africa']\n",
            "['Spring equinox', 'spring equinox', 'midrange', 'midrange', 'midrange', 'midrange', 'midrange', 'Midrange', 'Fall Equinox and Spring Equinox', 'fall equinox', 'fall equinox', 'Midrange', 'midrange', 'midrange', 'midrange', 'midrange', 'midrange between day and night', 'northern hemisphere', 'Summer solstice', 'June', 'southern', 'East Timor', 'southern hemisphere', 'Southern', 'southern hemisphere']\n",
            "time per query is  1.9667918491363525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Om10wxJN8UqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}